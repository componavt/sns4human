{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/componavt/sns4human/blob/main/src/vk/voyant_tools/post_tokens_with_group.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. This script reads a file with VK group post texts (created using the vk_group_all_posts.ipynb script).\n",
        "\n",
        "2. Tokenizes (breaks into words), saves periods in sentences.\n",
        "\n",
        "3. Combines posts from several VK groups into one CSV file. Adds a group field with the name of the VK group. Group posts are sorted by the date field.\n",
        "\n",
        "The resulting CSV file is ready to be uploaded to Voyant Tools. The text (the tokens field) is enclosed in quotation marks specifically for this purpose.\n",
        "\n",
        "---\n",
        "In Russian\n",
        "\n",
        "1. Этот скрипт читает файл с текстами постов группы ВК (созданный с помощью скрипта vk_group_all_posts.ipynb).\n",
        "\n",
        "2. Токенизирует (разбивает на слова), сохраняет точки в предложениях.\n",
        "\n",
        "3. Объединяет посты нескольких ВК-групп в один CSV-файл. Добавляет поле group с именем ВК-группы. Посты групп отсортированы по полю date.\n",
        "\n",
        "Итоговый CSV-файл готов к загрузке в Voyant Tools. Специально для этого текст (поле tokens) заключён в кавычки."
      ],
      "metadata": {
        "id": "x9qKY0fjJlXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# not religion\n",
        "# domains = ['club221681617','concerto','club151359929','pravoslav_karelia']# smallest groups for tests smallest_tokens.csv\n",
        "# domains = ['aparfenchikov','minnazrk', 'mincultrk', 'uoknrk']# state_tokens.csv\n",
        "# domains = ['rk_nationalmuseum','olonmus','museum_ptz','echo_association', 'domderevnivoknavolok', 'vepsmuseum', 'club226126304']# museum_tokens.csv\n",
        "domains = ['satasanaa','speechvepkar','desyatiletieyazykovkarelia','karjalan_kieli']# language_tokens.csv\n",
        "# domains = ['omapajo','kyykkakarjala','tastykarjala','senofest','olongames','kalevala_fest']# festival_tokens.csv\n",
        "# domains = ['karjalankielenkodi','mediacenter_periodika','public111906776','karjalanrahvahanliitto','club2562309', 'club_dk_padany', 'melnikpryazha']# multifunctional_tokens.csv\n",
        "\n",
        "# religion\n",
        "# domains = ['mitropolit_manuil','club57656949','popeshenie','ekaterinahram','nevsoborptz','sortavala_chram',\n",
        "#           'pravmk','club18647865','dpcentr','krest_sobor','stupeniorthodox','club103835710','club151359929','club221681617',\n",
        "#           'pravkarelia','svirskiymonastery','club2975745','club19347481','kemskoepodvorie']# orthodoxy.csv\n",
        "# domains = ['islamrk','halal_ptz','siogroups']# islam.csv\n",
        "# domains = ['infoinkeri','kemskij_prihod','club18959947','kirkko','kareliandiocese','concerto']# lutheran.csv\n",
        "# domains = ['hve10','church_of_christ_ptz','glorygod_ptz']# baptists_and_evangelists.csv\n",
        "result_file = 'language_tokens.csv'\n",
        "\n",
        "#religion = '/religion'\n",
        "religion = ''           # not religion\n",
        "\n",
        "# to archive or not result cvs file (cvs.gz)\n",
        "b_gzip = 1\n",
        "#b_gzip = 0"
      ],
      "metadata": {
        "id": "kLrxA1xm9Vzo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO6yySLtgsh2",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "!pip install -U pymorphy3\n",
        "import pymorphy3\n",
        "import requests\n",
        "import csv\n",
        "import gzip\n",
        "import shutil\n",
        "\n",
        "!pip install emoji\n",
        "import emoji\n",
        "\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words(\"russian\")\n",
        "stop_words += requests.get('https://raw.githubusercontent.com/componavt/sns4human/refs/heads/main/src/vk/nlp/RussianStopWords.txt').text.split('\\n')\n",
        "\n",
        "alphabet  = set('абвгдеёжзийклмнопрстуфхцчшщъыьэюя')                    # Russian alphabet\n",
        "alphabet |= set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ') # Add Latin letters (A-Z, a-z)\n",
        "alphabet |= set('äöåšžüÄÖÅŠŽÜ')  # ä, ö, å (Finnish); š, ž (Karelian); ü (Veps)\n",
        "alphabet_dash = alphabet | {'-'} # Optionally allow hyphen (dash) as part of words\n",
        "\n",
        "morph = pymorphy3.MorphAnalyzer(lang='ru')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_window(words, index, window_size=3):\n",
        "    \"\"\"Returns a context window of words around the given index.\"\"\"\n",
        "    start = max(0, index - window_size)\n",
        "    end = min(len(words), index + window_size + 1)\n",
        "    return ' '.join(words[start:end])\n",
        "\n",
        "def contains_non_dash(s):\n",
        "    \"\"\"Check if a string consists not only dash characters.\"\"\"\n",
        "    return s.count('-') < len(s)\n",
        "\n",
        "# if filter_fio then remove the last names from the text.\n",
        "# if period then then add a period at the end of each sentence.\n",
        "# ############################################################\n",
        "def process_text(text, filter_fio=False, period=False):\n",
        "    sentences = sent_tokenize(text)  # Split into sentences\n",
        "    processed_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        check_hash = False\n",
        "        processed_parts = []\n",
        "        words = word_tokenize(sentence)\n",
        "\n",
        "        for i, w in enumerate(words):\n",
        "          if len(w) == 1:\n",
        "            continue\n",
        "          if w == '#':\n",
        "            check_hash = True\n",
        "            continue\n",
        "          if check_hash:\n",
        "            check_hash = False\n",
        "            continue\n",
        "\n",
        "          # skip name and surname if filter_fio\n",
        "          if filter_fio:\n",
        "            w_tag = morph.parse(w.strip())[0].tag\n",
        "            if 'Surn' in w_tag or 'Name' in w_tag or 'Patr' in w_tag:\n",
        "              # context = get_text_window(words, i)\n",
        "              # print(f\"Filtered name/surname: {w} | Context: {context}\")  # Debug output for context\n",
        "              continue\n",
        "\n",
        "          if (set(w.lower()).issubset(alphabet_dash) and\n",
        "              contains_non_dash(w) and\n",
        "              \":\" not in emoji.demojize(w)):  # skip emojis\n",
        "\n",
        "              res = morph.parse(w.lower())[0].normal_form\n",
        "              if res and (res not in stop_words):\n",
        "                processed_parts.append(res)\n",
        "\n",
        "          else:\n",
        "              # If word contains >= 4 characters from the alphabet (e.g., mixed case or with punctuation)\n",
        "              if sum(1 for char in w.lower() if char in alphabet) >= 4:\n",
        "                  # skip words-hyperlinks\n",
        "                  if ('\\\\' not in w) and ('/' not in w) and (\":\" not in emoji.demojize(w)):\n",
        "                      #context = get_text_window(words, i)\n",
        "                      #print(f\"Filtered not subset(alphabet): {w} | Context: {context}\")\n",
        "                      res = morph.parse(w.lower())[0].normal_form\n",
        "                      if res and (res not in stop_words):\n",
        "                          processed_parts.append(res)\n",
        "\n",
        "        if processed_parts:\n",
        "            last_word = processed_parts[-1]\n",
        "            if last_word[-1] not in \".!?\" and period:\n",
        "                processed_parts[-1] += \".\"    # Attach period directly to the last word\n",
        "\n",
        "        processed_sentences.append(\" \".join(processed_parts))\n",
        "\n",
        "    return \" \".join(processed_sentences).strip()"
      ],
      "metadata": {
        "id": "Rb1DFsneHES-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_df = []\n",
        "for domain in domains:\n",
        "    t = pd.read_csv(f'https://raw.githubusercontent.com/componavt/sns4human/refs/heads/main/data/vk/posts{religion}/{domain}.csv',\n",
        "                    usecols=['text', 'date'])\n",
        "    t['group'] = domain\n",
        "    temp_df.append(t)\n",
        "\n",
        "df = pd.concat(temp_df, ignore_index=True).sort_values('date')\n",
        "df = df[df['text'].notna() & (df['text'].apply(lambda x: isinstance(x, str))) & (df['text'] != '')]\n",
        "\n",
        "df['tokens'] = df['text'].apply(process_text)\n",
        "df = df[df['tokens'].notna() & (df['tokens'].apply(lambda x: isinstance(x, str))) & (df['tokens'] != '') & (df['tokens'] != ' ')]\n",
        "df_tokens = pd.concat([df['tokens'], df['date'],df['group']], axis=1, keys=['tokens', 'date','group'])\n",
        "\n",
        "# Removing lines with empty 'tokens'\n",
        "df_tokens = df_tokens[df_tokens['tokens'].str.strip().astype(bool)]\n",
        "\n",
        "# Save CSV with quotes only for 'tokens' field, without quotes for 'date' and 'group'\n",
        "with open(result_file, 'w', newline='', encoding='utf-8') as f:\n",
        "#   writer = csv.writer(f, delimiter=';', quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
        "#   writer = csv.writer(f, delimiter=';', quoting=csv.QUOTE_NONE, quotechar=None)\n",
        "    writer = csv.writer(f, delimiter=',', quoting=csv.QUOTE_NONE, quotechar=None, escapechar='\\\\')\n",
        "    writer.writerow(['tokens', 'date', 'group'])\n",
        "    for _, row in df_tokens.iterrows():\n",
        "        writer.writerow([f'\"{row[\"tokens\"]}\"', row['date'], row['group']])\n",
        "\n",
        "# If b_gzip == 1, create a gzip archive\n",
        "if b_gzip == 1:\n",
        "    gzip_file = result_file + \".gz\"\n",
        "    with open(result_file, 'rb') as f_in, gzip.open(gzip_file, 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)\n",
        "    print(f\"Archived: {gzip_file}\")"
      ],
      "metadata": {
        "id": "1l1PJxzg0gEn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}