{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/componavt/sns4human/blob/main/src/vk/voyant_tools/post_tokens_with_group.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. This script reads a file with VK group post texts (created using the vk_group_all_posts.ipynb script).\n",
        "\n",
        "2. Tokenizes (breaks into words), saves periods in sentences.\n",
        "\n",
        "3. Combines posts from several VK groups into one CSV file. Adds a group field with the name of the VK group. Group posts are sorted by the date field.\n",
        "\n",
        "The resulting CSV file is ready to be uploaded to Voyant Tools. The text (the tokens field) is enclosed in quotation marks specifically for this purpose.\n",
        "\n",
        "---\n",
        "In Russian\n",
        "\n",
        "1. Этот скрипт читает файл с текстами постов группы ВК (созданный с помощью скрипта vk_group_all_posts.ipynb).\n",
        "\n",
        "2. Токенизирует (разбивает на слова), сохраняет точки в предложениях.\n",
        "\n",
        "3. Объединяет посты нескольких ВК-групп в один CSV-файл. Добавляет поле group с именем ВК-группы. Посты групп отсортированы по полю date.\n",
        "\n",
        "Итоговый CSV-файл готов к загрузке в Voyant Tools. Специально для этого текст (поле tokens) заключён в кавычки."
      ],
      "metadata": {
        "id": "x9qKY0fjJlXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# not religion\n",
        "# domains = ['club221681617','concerto','club151359929','pravoslav_karelia']# smallest groups for tests smallest_tokens.csv\n",
        "domains = ['aparfenchikov','minnazrk', 'mincultrk', 'uoknrk']# state_tokens.csv\n",
        "# domains = ['rk_nationalmuseum','olonmus','museum_ptz','echo_association', 'domderevnivoknavolok', 'vepsmuseum', 'club226126304']# museum_tokens.csv\n",
        "# domains = ['satasanaa','speechvepkar','desyatiletieyazykovkarelia','karjalan_kieli']# language_tokens.csv\n",
        "# domains = ['omapajo','kyykkakarjala','tastykarjala','senofest','olongames','kalevala_fest']# festival_tokens.csv\n",
        "# domains = ['karjalankielenkodi','mediacenter_periodika','public111906776','karjalanrahvahanliitto','club2562309', 'club_dk_padany', 'melnikpryazha']# multifunctional_tokens.csv\n",
        "\n",
        "# religion\n",
        "# domains = ['mitropolit_manuil','club57656949','popeshenie','ekaterinahram','nevsoborptz','sortavala_chram',\n",
        "#           'pravmk','club18647865','dpcentr','krest_sobor','stupeniorthodox','club103835710','club151359929','club221681617',\n",
        "#           'pravkarelia','svirskiymonastery','club2975745','club19347481','kemskoepodvorie']# orthodoxy.csv\n",
        "# domains = ['islamrk','halal_ptz','siogroups']# islam.csv\n",
        "# domains = ['infoinkeri','kemskij_prihod','club18959947','kirkko','kareliandiocese','concerto']# lutheran.csv\n",
        "# domains = ['hve10','church_of_christ_ptz','glorygod_ptz']# baptists_and_evangelists.csv\n",
        "result_file = 'state_tokens.csv'\n",
        "\n",
        "#religion = '/religion'\n",
        "religion = ''           # not religion\n",
        "\n",
        "# to archive or not result cvs file (cvs.gz)\n",
        "b_gzip = 1\n",
        "#b_gzip = 0"
      ],
      "metadata": {
        "id": "kLrxA1xm9Vzo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PO6yySLtgsh2",
        "outputId": "c5cbc565-8052-4f3a-bd24-b41c3f8908a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (2.4.417150.4580142)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.14.1)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "!pip install -U pymorphy3\n",
        "import pymorphy3\n",
        "import requests\n",
        "import csv\n",
        "import gzip\n",
        "import shutil\n",
        "\n",
        "!pip install emoji\n",
        "import emoji\n",
        "\n",
        "from io import StringIO\n",
        "filename = 'text_preprocessing.py'\n",
        "response = requests.get(f'https://raw.githubusercontent.com/componavt/sns4human/refs/heads/main/src/vk/nlp/{filename}')\n",
        "with open(filename,'w+') as f:\n",
        "  f.write(response.text)\n",
        "import text_preprocessing\n",
        "\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words(\"russian\")\n",
        "stop_words += requests.get('https://raw.githubusercontent.com/componavt/sns4human/refs/heads/main/src/vk/nlp/RussianStopWords.txt').text.split('\\n')\n",
        "\n",
        "alphabet  = set('абвгдеёжзийклмнопрстуфхцчшщъыьэюя')                    # Russian alphabet\n",
        "alphabet |= set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ') # Add Latin letters (A-Z, a-z)\n",
        "alphabet |= set('äöåšžüÄÖÅŠŽÜ')  # ä, ö, å (Finnish); š, ž (Karelian); ü (Veps)\n",
        "alphabet_dash = alphabet | {'-'} # Optionally allow hyphen (dash) as part of words\n",
        "\n",
        "morph = pymorphy3.MorphAnalyzer(lang='ru')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_df = []\n",
        "for domain in domains:\n",
        "    t = pd.read_csv(f'https://raw.githubusercontent.com/componavt/sns4human/refs/heads/main/data/vk/posts{religion}/{domain}.csv',\n",
        "                    usecols=['text', 'date'])\n",
        "    t['group'] = domain\n",
        "    temp_df.append(t)\n",
        "\n",
        "df = pd.concat(temp_df, ignore_index=True).sort_values('date')\n",
        "df = df[df['text'].notna() & (df['text'].apply(lambda x: isinstance(x, str))) & (df['text'] != '')]\n",
        "\n",
        "df['tokens'] = df['text'].apply(lambda x: text_preprocessing.process_text(x, filter_fio=False, period=True))\n",
        "\n",
        "df = df[df['tokens'].notna() & (df['tokens'].apply(lambda x: isinstance(x, str))) & (df['tokens'] != '') & (df['tokens'] != ' ')]\n",
        "df_tokens = pd.concat([df['tokens'], df['date'],df['group']], axis=1, keys=['tokens', 'date','group'])\n",
        "\n",
        "# Removing lines with empty 'tokens'\n",
        "df_tokens = df_tokens[df_tokens['tokens'].str.strip().astype(bool)]\n",
        "\n",
        "# Save CSV with quotes only for 'tokens' field, without quotes for 'date' and 'group'\n",
        "with open(result_file, 'w', newline='', encoding='utf-8') as f:\n",
        "#   writer = csv.writer(f, delimiter=';', quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
        "#   writer = csv.writer(f, delimiter=';', quoting=csv.QUOTE_NONE, quotechar=None)\n",
        "    writer = csv.writer(f, delimiter=',', quoting=csv.QUOTE_NONE, quotechar=None, escapechar='\\\\')\n",
        "    writer.writerow(['tokens', 'date', 'group'])\n",
        "    for _, row in df_tokens.iterrows():\n",
        "        writer.writerow([f'\"{row[\"tokens\"]}\"', row['date'], row['group']])\n",
        "\n",
        "# If b_gzip == 1, create a gzip archive\n",
        "if b_gzip == 1:\n",
        "    gzip_file = result_file + \".gz\"\n",
        "    with open(result_file, 'rb') as f_in, gzip.open(gzip_file, 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)\n",
        "    print(f\"Archived: {gzip_file}\")"
      ],
      "metadata": {
        "id": "1l1PJxzg0gEn",
        "outputId": "f13d1635-33bb-49e8-e4fb-bc73ec99c641",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archived: state_tokens.csv.gz\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}