{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/componavt/sns4human/blob/main/src/vk/voyant_tools/post_tokens_with_group.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. This script reads a file with VK group post texts (created using the vk_group_all_posts.ipynb script).\n",
        "\n",
        "2. Tokenizes (breaks into words), saves periods in sentences.\n",
        "\n",
        "3. Combines posts from several VK groups into one CSV file. Adds a group field with the name of the VK group. Group posts are sorted by the date field.\n",
        "\n",
        "The resulting CSV file is ready to be uploaded to Voyant Tools. The text (the tokens field) is enclosed in quotation marks specifically for this purpose.\n",
        "\n",
        "---\n",
        "In Russian\n",
        "\n",
        "1. Этот скрипт читает файл с текстами постов группы ВК (созданный с помощью скрипта vk_group_all_posts.ipynb).\n",
        "\n",
        "2. Токенизирует (разбивает на слова), сохраняет точки в предложениях.\n",
        "\n",
        "3. Объединяет посты нескольких ВК-групп в один CSV-файл. Добавляет поле group с именем ВК-группы. Посты групп отсортированы по полю date.\n",
        "\n",
        "Итоговый CSV-файл готов к загрузке в Voyant Tools. Специально для этого текст (поле tokens) заключён в кавычки."
      ],
      "metadata": {
        "id": "x9qKY0fjJlXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "domains = ['club221681617','concerto','club151359929','pravoslav_karelia']# smallest groups for tests\n",
        "result_file = 'tokens_group.csv'"
      ],
      "metadata": {
        "id": "kLrxA1xm9Vzo"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "PO6yySLtgsh2",
        "outputId": "34bda907-43c5-4240-b119-578baa47f247",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (2.4.417150.4580142)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "!pip install -U pymorphy3\n",
        "import pymorphy3\n",
        "import requests\n",
        "import csv\n",
        "\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words(\"russian\")\n",
        "\n",
        "alphabet = set('абвгдеёжзийклмнопрстуфхцчшщъыьэюя')\n",
        "alphabet_dash = alphabet | {'-'}# alphabet + dash\n",
        "\n",
        "morph = pymorphy3.MorphAnalyzer(lang='ru')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_window(words, index, window_size=3):\n",
        "    \"\"\"Returns a context window of words around the given index.\"\"\"\n",
        "    start = max(0, index - window_size)\n",
        "    end = min(len(words), index + window_size + 1)\n",
        "    return ' '.join(words[start:end])\n",
        "\n",
        "def contains_non_dash(s):\n",
        "    \"\"\"Check if a string consists not only dash characters.\"\"\"\n",
        "    return s.count('-') < len(s)\n",
        "\n",
        "def process_text(text):\n",
        "    sentences = sent_tokenize(text)  # Split into sentences\n",
        "    processed_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        check_hash = False\n",
        "        processed_parts = []\n",
        "        words = word_tokenize(sentence)\n",
        "\n",
        "        for i, w in enumerate(words):\n",
        "          if len(w) == 1:\n",
        "            continue\n",
        "          if w == '#':\n",
        "            check_hash = True\n",
        "            continue\n",
        "          if check_hash:\n",
        "            check_hash = False\n",
        "            continue\n",
        "\n",
        "          # skip name and surname\n",
        "          # w_tag = morph.parse(w.strip())[0].tag\n",
        "          #if 'Surn' in w_tag or 'Name' in w_tag or 'Patr' in w_tag:\n",
        "          #  context = get_text_window(words, i)\n",
        "          #  print(f\"Filtered name/surname: {w} | Context: {context}\")  # Debug output for context\n",
        "          #  continue\n",
        "\n",
        "          if set(w.lower()).issubset(alphabet_dash) and contains_non_dash(w):\n",
        "            res = morph.parse(w.lower())[0].normal_form\n",
        "            if res and (res not in stop_words):\n",
        "                  processed_parts.append(res)\n",
        "          else:\n",
        "            # has 4+ Cyrillic characters then will parse too (e.g. блж.Фаддея о.Алексия г.Петрозаводске)\n",
        "            if sum(1 for char in w.lower() if char in alphabet) >= 4:\n",
        "              if ('\\\\' not in w) and ('/' not in w): # skip words-hyperlinks\n",
        "                #context = get_text_window(words, i)\n",
        "                #print(f\"Filtered not subset(alphabet): {w} | Context: {context}\")\n",
        "                res = morph.parse(w.lower())[0].normal_form\n",
        "                if res not in stop_words:\n",
        "                  processed_parts.append(res)\n",
        "\n",
        "        if processed_parts:\n",
        "            last_word = processed_parts[-1]\n",
        "            if last_word[-1] not in \".!?\":\n",
        "                processed_parts.append(\".\")  # Add period at the end of sentence\n",
        "\n",
        "        processed_sentences.append(\" \".join(processed_parts))\n",
        "\n",
        "    return \" \".join(processed_sentences)"
      ],
      "metadata": {
        "id": "Rb1DFsneHES-"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_df = []\n",
        "for domain in domains:\n",
        "    t = pd.read_csv(f'https://raw.githubusercontent.com/componavt/sns4human/refs/heads/main/data/vk/posts/religion/{domain}.csv',\n",
        "                    usecols=['text', 'date'])\n",
        "    t['group'] = domain\n",
        "    temp_df.append(t)\n",
        "\n",
        "df = pd.concat(temp_df, ignore_index=True).sort_values('date')\n",
        "df = df[df['text'].notna() & (df['text'].apply(lambda x: isinstance(x, str))) & (df['text'] != '')]\n",
        "\n",
        "df['tokens'] = df['text'].apply(process_text)\n",
        "df = df[df['tokens'].notna() & (df['tokens'].apply(lambda x: isinstance(x, str))) & (df['tokens'] != '') & (df['tokens'] != ' ')]\n",
        "df_tokens = pd.concat([df['tokens'], df['date'],df['group']], axis=1, keys=['tokens', 'date','group'])\n",
        "\n",
        "# Removing lines with empty 'tokens'\n",
        "df_tokens = df_tokens[df_tokens['tokens'].str.strip().astype(bool)]\n",
        "\n",
        "# Save CSV with quotes only for 'tokens' field, without quotes for 'date' and 'group'\n",
        "with open(result_file, 'w', newline='', encoding='utf-8') as f:\n",
        "#    writer = csv.writer(f, delimiter=';', quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
        "    writer = csv.writer(f, delimiter=';', quoting=csv.QUOTE_NONE, quotechar=None)\n",
        "    writer.writerow(['tokens', 'date', 'group'])\n",
        "    for _, row in df_tokens.iterrows():\n",
        "        writer.writerow([f'\"{row[\"tokens\"]}\"', row['date'], row['group']])"
      ],
      "metadata": {
        "id": "1l1PJxzg0gEn"
      },
      "execution_count": 40,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}