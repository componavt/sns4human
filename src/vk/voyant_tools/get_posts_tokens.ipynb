{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/componavt/sns4human/blob/main/src/vk/voyant_tools/get_posts_tokens.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script processes text data, tokenizes it, and saves it to a CSV file.\n",
        "For [Voyant Tools](https://voyant-tools.org/), periods in sentences are preserved.\n",
        "It filters out entries with empty 'tokens' fields and ensures 'tokens' are always quoted, while 'date' remains unquoted.\n",
        "\n",
        "Этот скрипт обрабатывает текстовые данные, токенизирует их и сохраняет в CSV-файл.\n",
        "Для Voyant Tools сохраняются точки в предложениях.\n",
        "Он отфильтровывает записи с пустыми полями «токены» и гарантирует, что «токены» всегда заключены в кавычки, а «дата» остается без кавычек."
      ],
      "metadata": {
        "id": "2e4e1oWmQZbv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO6yySLtgsh2",
        "outputId": "d40abaa5-2d5a-42f4-94ab-f9938e7deb8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (2.4.417150.4580142)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "!pip install -U pymorphy3\n",
        "import pymorphy3\n",
        "import requests\n",
        "import csv\n",
        "\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words(\"russian\")\n",
        "stop_words += requests.get('https://raw.githubusercontent.com/componavt/sns4human/refs/heads/main/src/vk/nlp/RussianStopWords.txt').text.split('\\n')\n",
        "stop_words += requests.get('https://raw.githubusercontent.com/componavt/sns4human/refs/heads/main/src/vk/nlp/stopwords-ru.txt').text.split()\n",
        "\n",
        "alphabet = set('абвгдеёжзийклмнопрстуфхцчшщъыьэюя')\n",
        "alphabet_dash = set('абвгдеёжзийклмнопрстуфхцчшщъыьэюя-')\n",
        "\n",
        "morph = pymorphy3.MorphAnalyzer(lang='ru')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjsktO3-kRBW"
      },
      "outputs": [],
      "source": [
        "domains = ['club221681617','concerto','club151359929','pravoslav_karelia']# smallest groups for tests\n",
        "df = pd.concat([pd.read_csv('https://raw.githubusercontent.com/componavt/sns4human/refs/heads/main/data/vk/posts/religion/' + domain +'.csv',\n",
        "                            usecols = ['text','date']) for domain in domains], ignore_index=True)\n",
        "df = df[df['text'].notna() & (df['text'].apply(lambda x: isinstance(x, str)))]\n",
        "df = df.reset_index()\n",
        "df = df.drop(columns=['index'])\n",
        "\n",
        "def get_text_window(words, index, window_size=3):\n",
        "    \"\"\"Returns a context window of words around the given index.\"\"\"\n",
        "    start = max(0, index - window_size)\n",
        "    end = min(len(words), index + window_size + 1)\n",
        "    return ' '.join(words[start:end])\n",
        "\n",
        "def contains_non_dash(s):\n",
        "    \"\"\"Check if a string consists not only dash characters.\"\"\"\n",
        "    return s.count('-') < len(s)\n",
        "\n",
        "def process_text(text):\n",
        "    sentences = sent_tokenize(text)  # Split into sentences\n",
        "    processed_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        check_hash = False\n",
        "        processed_parts = []\n",
        "        words = word_tokenize(sentence)\n",
        "\n",
        "        for i, w in enumerate(words):\n",
        "          if len(w) == 1:\n",
        "            continue\n",
        "          if w == '#':\n",
        "            check_hash = True\n",
        "            continue\n",
        "          if check_hash:\n",
        "            check_hash = False\n",
        "            continue\n",
        "\n",
        "          # skip name and surname\n",
        "          # w_tag = morph.parse(w.strip())[0].tag\n",
        "          #if 'Surn' in w_tag or 'Name' in w_tag or 'Patr' in w_tag:\n",
        "          #  context = get_text_window(words, i)\n",
        "          #  print(f\"Filtered name/surname: {w} | Context: {context}\")  # Debug output for context\n",
        "          #  continue\n",
        "\n",
        "          if set(w.lower()).issubset(alphabet_dash) and contains_non_dash(w):\n",
        "            res = morph.parse(w.lower())[0].normal_form\n",
        "            if res and (res not in stop_words):\n",
        "                  processed_parts.append(res)\n",
        "          else:\n",
        "            # has 4+ Cyrillic characters then will parse too (e.g. блж.Фаддея о.Алексия г.Петрозаводске)\n",
        "            if sum(1 for char in w.lower() if char in alphabet) >= 4:\n",
        "              if ('\\\\' not in w) and ('/' not in w): # skip words-hyperlinks\n",
        "                #context = get_text_window(words, i)\n",
        "                #print(f\"Filtered not subset(alphabet): {w} | Context: {context}\")\n",
        "                res = morph.parse(w.lower())[0].normal_form\n",
        "                if res not in stop_words:\n",
        "                  processed_parts.append(res)\n",
        "\n",
        "        if processed_parts:\n",
        "            last_word = processed_parts[-1]\n",
        "            if last_word[-1] not in \".!?\":\n",
        "                processed_parts.append(\".\")  # Add period at the end of sentence\n",
        "\n",
        "        processed_sentences.append(\" \".join(processed_parts))\n",
        "\n",
        "    return \" \".join(processed_sentences)\n",
        "\n",
        "df['tokens'] = df['text'].apply(process_text)\n",
        "df_tokens = df[['tokens', 'date']]\n",
        "\n",
        "# Removing lines with empty 'tokens'\n",
        "df_tokens = df_tokens[df_tokens['tokens'].str.strip().astype(bool)]\n",
        "\n",
        "# Save CSV with quotes only for 'tokens' field, without quotes for 'date'\n",
        "with open('tokens.csv', 'w', newline='', encoding='utf-8') as f:\n",
        "    writer = csv.writer(f, delimiter=';', quoting=csv.QUOTE_NONE, quotechar=None)\n",
        "    writer.writerow(['tokens', 'date'])\n",
        "    for _, row in df_tokens.iterrows():\n",
        "        writer.writerow([f'\"{row[\"tokens\"]}\"', row['date']])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}