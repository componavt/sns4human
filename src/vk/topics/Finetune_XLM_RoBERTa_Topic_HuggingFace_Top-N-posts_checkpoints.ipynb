{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMA9sUWiL14QQM9ynAH6/Qi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/componavt/sns4human/blob/main/src/vk/topics/Finetune_XLM_RoBERTa_Topic_HuggingFace_Top-N-posts_checkpoints.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Social Media Posts Topic Classification üóÇÔ∏èüîç\n",
        "\n",
        "## English ‚òï\n",
        "\n",
        "This script performs automated topic classification of social media posts using a fine-tuned XLM-RoBERTa model. Key features include:\n",
        "\n",
        "- ‚ö° **Parallel batch processing** for high-speed classification\n",
        "- üìå **Automatic checkpointing** to resume progress after interruptions  \n",
        "- üéØ **Top-k filtering** to save only the most relevant posts per topic\n",
        "\n",
        "### Implementation Details\n",
        "\n",
        "The script uses our pre-trained [`xlm-roberta-base-topic-classification-2025`](https://huggingface.co/componavt/xlm-roberta-base-topic-classification-2025) model from Hugging Face. It processes CSV files from the [`sns4human/data/vk/posts/`](https://github.com/componavt/sns4human/tree/main/data/vk/posts) directory.\n",
        "\n",
        "For each post:\n",
        "1. Classifies content into expert-defined topics\n",
        "2. Calculates a confidence score (0-100%)\n",
        "3. Maintains leaderboards of top posts per topic\n",
        "\n",
        "Results are saved as separate CSV files (one per topic) containing only the highest-confidence posts. Real-time progress is displayed in the notebook.\n",
        "\n",
        "> **Note:** The script automatically handles GPU/CPU detection and optimizes batch size accordingly.\n",
        "\n",
        "## –†—É—Å—Å–∫–∏–π ü´ñ\n",
        "\n",
        "–°–∫—Ä–∏–ø—Ç –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –ø–æ—Å—Ç–æ–≤. –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏: ‚ö° –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è, üìå –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –ø–æ—Å–ª–µ —Å–±–æ–µ–≤ –∏ üéØ –æ—Ç–±–æ—Ä —Ç–æ–ø-k –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã.\n",
        "\n",
        "–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –∑–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å [XLM-RoBERTa](https://huggingface.co/componavt/xlm-roberta-base-topic-classification-2025) (`xlm-roberta-base-topic-classification-2025`) –∏–∑ Hugging Face –¥–ª—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –°–∫—Ä–∏–ø—Ç —á–∏—Ç–∞–µ—Ç CSV-—Ñ–∞–π–ª—ã –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ [`./data/vk/posts/`](https://github.com/componavt/sns4human/tree/main/data/vk/posts). –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ—Å—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º, –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–º —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ —Ç–∞–∫: –∫–∞–∂–¥–æ–º—É –ø–æ—Å—Ç—É –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç—Å—è —Ç–µ–º–∞ –∏ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è —Å—Ç–µ–ø–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏. –î–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ `k` –ø–æ—Å—Ç–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ CSV-—Ñ–∞–π–ª—ã."
      ],
      "metadata": {
        "id": "HT0oibt5xB4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of CSV files to process (change as needed)\n",
        "n_files = 999 # 2 999\n",
        "\n",
        "# Top-N posts per topic to keep\n",
        "k_top = 1000    # 10 30 1000\n",
        "\n",
        "print_every_n = 100\n",
        "\n",
        "# Define model path (Hugging Face repo)\n",
        "model_repo = \"componavt/xlm-roberta-base-topic-classification-2025\"\n",
        "\n",
        "batch_size = 32 if torch.cuda.is_available() else 8\n",
        "\n",
        "checkpoint_path = \"checkpoint.json\""
      ],
      "metadata": {
        "id": "Xupcx1PrrBGY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load or initialize checkpoint\n",
        "def load_checkpoint(path):\n",
        "    if not os.path.exists(path): return set(), defaultdict(list)\n",
        "    with open(path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "        heap = defaultdict(list)\n",
        "        for topic, items in data[\"top_k_heap\"].items():\n",
        "            for score, uid, record in items:\n",
        "                heapq.heappush(heap[topic], (score, uid, record))\n",
        "        return set(data[\"processed_files\"]), heap\n",
        "\n",
        "def save_checkpoint(processed, heap, path):\n",
        "    serializable = {\n",
        "        \"processed_files\": list(processed),\n",
        "        \"top_k_heap\": {\n",
        "            topic: [(score, uid, rec) for score, uid, rec in entries]\n",
        "            for topic, entries in heap.items()\n",
        "        }\n",
        "    }\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(serializable, f)\n",
        "\n",
        "# Batched inference\n",
        "def process_batch(texts):\n",
        "    try:\n",
        "        return pipe(texts, batch_size=len(texts))\n",
        "    except Exception as e:\n",
        "        print(f\"Batch error: {e}\")\n",
        "        return [None] * len(texts)"
      ],
      "metadata": {
        "id": "alv2wt8umqL5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Inference with Fine-Tuned Model from Hugging Face Hub\n",
        "\n",
        "!pip install -U transformers pandas scikit-learn\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "import heapq\n",
        "from collections import defaultdict\n",
        "from itertools import count\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_repo)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
        "\n",
        "# Set model to GPU if available\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "pipe = TextClassificationPipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    top_k=None,\n",
        "    device=device,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "# Define the topic list (same as during training)\n",
        "topic_labels = [\n",
        "    \"–±–ª–∞–≥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\", \"–≤–µ–ø—Å—ã\", \"–≤–æ–π–Ω–∞\", \"–≤—ã—Å—Ç–∞–≤–∫–∞\",\n",
        "    \"–¥–µ–Ω—å —Ä–æ–∂–¥–µ–Ω–∏—è –∏ —é–±–∏–ª–µ–π\", \"–µ–¥–∞ –∫—É—Ö–Ω—è\", \"–∫–∞—Ä–µ–ª—å—Å–∫–∏–π —è–∑—ã–∫\",\n",
        "    \"–∫–æ–Ω–∫—É—Ä—Å\", \"–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –ø—Ä–æ–µ–∫—Ç—ã\", \"–º—É–∑–µ–π. —ç–∫—Å–∫—É—Ä—Å–∏—è\",\n",
        "    \"–º—É–∑—ã–∫–∞\", \"–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ\", \"–ø—Ä–∞–∑–¥–Ω–∏–∫\", \"–ø—Ä–∏—Ä–æ–¥–∞\",\n",
        "    \"–ø–æ—ç–∑–∏—è –ö–∞–ª–µ–≤–∞–ª–∞ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞\", \"—Å–æ—Ü–∏–∞–ª—å–Ω–∞—è —Å—Ñ–µ—Ä–∞\",\n",
        "    \"—Ç—Ä–∞–¥–∏—Ü–∏—è\", \"—Ñ–µ—Å—Ç–∏–≤–∞–ª—å\", \"—ç—Ç–Ω–æ–∫—É–ª—å—Ç—É—Ä–∞ —Ñ–æ–ª—å–∫–ª–æ—Ä\",\n",
        "    \"—è–∑—ã–∫\", \"–≠–ø–æ—Å_–ö–∞–ª–µ–≤–∞–ª–∞\", \"–ø–æ—ç–∑–∏—è\", \"–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞\"\n",
        "]\n",
        "\n",
        "# Fit label encoder for inverse mapping\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(topic_labels)\n",
        "\n",
        "# Clone the GitHub repo and collect CSVs\n",
        "if not os.path.exists(\"sns4human\"):\n",
        "    !git clone https://github.com/componavt/sns4human.git\n",
        "posts_dir = \"sns4human/data/vk/posts\"\n",
        "\n",
        "csv_files = [f for f in os.listdir(posts_dir) if f.endswith(\".csv\")][:n_files]\n",
        "\n",
        "# Dictionary of min-heaps for each topic\n",
        "processed_files, top_k_heap = load_checkpoint(checkpoint_path)\n",
        "\n",
        "unique_counter = count()\n",
        "row_counter = 0\n",
        "\n",
        "for file in tqdm(csv_files, desc=\"Processing files\"):\n",
        "    if file in processed_files:\n",
        "        continue\n",
        "\n",
        "    df = pd.read_csv(os.path.join(posts_dir, file), encoding=\"utf-8\")\n",
        "    texts, rows = [], []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        text = row.get(\"text\", \"\")\n",
        "        if isinstance(text, str) and text.strip():\n",
        "            texts.append(text)\n",
        "            rows.append(row)\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        batch_rows = rows[i:i+batch_size]\n",
        "        batch_results = process_batch(batch)\n",
        "\n",
        "        for result, row in zip(batch_results, batch_rows):\n",
        "            row_counter += 1\n",
        "            if not result: continue\n",
        "\n",
        "            try:\n",
        "                best = max(result, key=lambda x: x[\"score\"])\n",
        "                pred_label = int(best[\"label\"].replace(\"LABEL_\", \"\"))\n",
        "                topic = label_encoder.inverse_transform([pred_label])[0]\n",
        "                score = round(best[\"score\"], 4)\n",
        "                heap = top_k_heap[topic]\n",
        "\n",
        "                record = {\n",
        "                    \"topic\": topic,\n",
        "                    \"relatedness\": score,\n",
        "                    \"text\": row.get(\"text\"),\n",
        "                    \"date\": row.get(\"date\"),\n",
        "                    \"group\": row.get(\"group\"),\n",
        "                    \"likes\": row.get(\"likes\"),\n",
        "                    \"reposts\": row.get(\"reposts\"),\n",
        "                    \"views\": row.get(\"views\")\n",
        "                }\n",
        "\n",
        "                if len(heap) < k_top:\n",
        "                    heapq.heappush(heap, (score, next(unique_counter), record))\n",
        "                    print(f\"1: {topic} (confidence: {score:.2%}) | {record['text'][:50]}...\") if row_counter % print_every_n == 0 else None\n",
        "                elif score > heap[0][0]:\n",
        "                    heapq.heappushpop(heap, (score, next(unique_counter), record))\n",
        "                    print(f\"2: {topic} (confidence: {score:.2%}) | {record['text'][:50]}...\") if row_counter % print_every_n == 0 else None\n",
        "            except Exception as e:\n",
        "                print(f\"Error in row: {e}\")\n",
        "\n",
        "    processed_files.add(file)\n",
        "    save_checkpoint(processed_files, top_k_heap, checkpoint_path)\n",
        "\n",
        "# Save final results\n",
        "for topic, heap in top_k_heap.items():\n",
        "    sorted_records = [rec for _, _, rec in sorted(heap, key=lambda x: -x[0])]\n",
        "    pd.DataFrame(sorted_records).to_csv(f\"{topic}.csv\", index=False, encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "DTPaOuoim6l6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}