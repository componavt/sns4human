{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPk7Uz07IHIG9SqdbkncIVI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/componavt/sns4human/blob/main/src/vk/topics/Finetune_XLM_RoBERTa_Topic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input files\n",
        "file_expert_labeled = \"512_posts_24_topics.csv\"  # expert-labeled dataset\n",
        "unlabeled_posts = \"speechvepkar.csv\"\n",
        "\n",
        "# see 25 XLM-RoBERTa models: Russian + text classification: https://huggingface.co/models?pipeline_tag=text-classification&language=ru&sort=trending&search=XLM-RoBERTa\n",
        "\n",
        "model_name = \"xlm-roberta-base\" #üëå (125M+ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)\n",
        "# OK: max_length=256\n",
        "# OK: per_device_train_batch_size=8\n",
        "# OK: per_device_eval_batch_size =8\n",
        "\n",
        "# model_name = \"mrm8488/XLM-RoBERTa-tiny\" # failed to load –£—Ä–µ–∑–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è XLM-R —Å –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π üîΩ 22–ú\n",
        "# model_name = \"mrm8488/xlm-roberta-base-finetuned-HC3-mix\" # –º–æ–¥–µ–ª—å —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—É—é \"–≥–æ–ª–æ–≤—É\" (head) —Å 2 –≤—ã—Ö–æ–¥–∞–º–∏, –ø–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω–∞ –æ–±—É—á–∞–ª–∞—Å—å –Ω–∞ –¥–≤–æ–∏—á–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, truthful / deceptive)\n",
        "# model_name = \"DeepPavlov/xlm-roberta-large-en-ru-mnli\" # (head) —Å 3 –≤—ã—Ö–æ–¥–∞–º–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏, —Ä–∞–∑–º–µ—Ä [3, 1024] (3 –∫–ª–∞—Å—Å–∞), –∞ –Ω–µ 23\n",
        "\n",
        "# model_name = \"cointegrated/rubert-tiny\" #üëå–ö—Ä–∞–π–Ω–µ –Ω–∏–∑–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ (Accuracy 21.5%, F1 ~4‚Äì9%) ‚Äî –º–æ–¥–µ–ª—å –ø–æ—á—Ç–∏ –Ω–µ –æ–±—É—á–∞–µ—Ç—Å—è.\n",
        "# model_name = \"distilbert-base-multilingual-cased\" #üëå–º–µ—Ç—Ä–∏–∫–∏ –Ω–µ–ø—Ä–∏–µ–º–ª–µ–º–æ –Ω–∏–∑–∫–∏–µ (Accuracy ~25%, F1 < 15%).\n",
        "                                                    # –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π DistilBERT —Å –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ—Å—Ç—å—é\tüîΩ 134–ú\n",
        "# model_name = \"papluca/xlm-roberta-base-language-detection\" # failed, since language-detection\n",
        "\n",
        "# –£–º–µ–Ω—å—à–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è 3, 2, 1\n",
        "num_train_epochs=21 # 3 10\n",
        "\n",
        "# –£–º–µ–Ω—å—à–∏—Ç—å max_length –ø—Ä–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (todo: –ø–æ–¥—Å—á–∏—Ç–∞—Ç—å –¥–ª–∏–Ω—É –ø–æ—Å—Ç–æ–≤)\n",
        "max_length=512\n",
        "# max_length=256\n",
        "# max_length=128\n",
        "\n",
        "learning_rate=2e-5 # (0.00002) –ú–µ–Ω—å—à–∏–π learning rate –æ–∑–Ω–∞—á–∞–µ—Ç –±–æ–ª–µ–µ –º–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏. –≠—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–º—É, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é.\n",
        "# learning_rate=3e-5 # (0.00003) –ß—É—Ç—å –±–æ–ª—å—à–∏–π learning rate —É—Å–∫–æ—Ä—è–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤, —á—Ç–æ –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å –º–æ–¥–µ–ª–∏ –±—ã—Å—Ç—Ä–µ–µ —Å—Ö–æ–¥–∏—Ç—å—Å—è, –Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —Ä–∏—Å–∫ \"–ø–µ—Ä–µ—Å–∫–∞–∫–∏–≤–∞–Ω–∏—è\" –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è.\n",
        "\n",
        "per_device_train_batch_size=16\n",
        "per_device_eval_batch_size =16\n",
        "#per_device_train_batch_size=8\n",
        "#per_device_eval_batch_size =8\n"
      ],
      "metadata": {
        "id": "DTPaOuoim6l6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGVGyYP4mrUm",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Fine-tune XLM-RoBERTa for topic classification on Russian social media posts (GPU-friendly üß†‚ö°)\n",
        "\n",
        "!pip install -U transformers datasets scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "from io import StringIO\n",
        "import requests\n",
        "\n",
        "# Load data labeled by expert from GitHub\n",
        "response = requests.get(f'https://raw.githubusercontent.com/componavt/sns4human/refs/heads/main/data/vk/topics/{file_expert_labeled}')\n",
        "df = pd.read_csv(StringIO(response.text), delimiter=',', encoding='utf-8')\n",
        "df = df[df['topic'].str.lower() != '–ø—É—Å—Ç–æ'].copy()\n",
        "\n",
        "#if model_name == \"papluca/xlm-roberta-base-language-detection\":\n",
        "    # Merge expert-assigned topics\n",
        "    # (1) –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤ –æ–¥–Ω—É —Ç–µ–º—É \"–ø–æ—ç–∑–∏—è –ö–∞–ª–µ–≤–∞–ª–∞ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞\" —Ç—Ä–∏ —Ç–µ–º—ã: \"–ø–æ—ç–∑–∏—è\", \"–≠–ø–æ—Å_–ö–∞–ª–µ–≤–∞–ª–∞\", \"–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞\";\n",
        "    # (2) –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤ –æ–¥–Ω—É —Ç–µ–º—É \"—ç—Ç–Ω–æ–∫—É–ª—å—Ç—É—Ä–∞ —Ñ–æ–ª—å–∫–ª–æ—Ä\" –¥–≤–µ —Ç–µ–º—ã: \"—ç—Ç–Ω–æ–∫—É–ª—å—Ç—É—Ä–∞\" –∏ \"—Ñ–æ–ª—å–∫–ª–æ—Ä\". –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ç–µ–º —Å—Ç–∞–Ω–µ—Ç –≤–º–µ—Å—Ç–æ 23 –±—É–¥–µ—Ç 20.\n",
        "#    merge_map = {\n",
        "#        '–ø–æ—ç–∑–∏—è': '–ø–æ—ç–∑–∏—è –ö–∞–ª–µ–≤–∞–ª–∞ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞',\n",
        "#        '–≠–ø–æ—Å_–ö–∞–ª–µ–≤–∞–ª–∞': '–ø–æ—ç–∑–∏—è –ö–∞–ª–µ–≤–∞–ª–∞ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞',\n",
        "#        '–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞': '–ø–æ—ç–∑–∏—è –ö–∞–ª–µ–≤–∞–ª–∞ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞',\n",
        "#        '—ç—Ç–Ω–æ–∫—É–ª—å—Ç—É—Ä–∞': '—ç—Ç–Ω–æ–∫—É–ª—å—Ç—É—Ä–∞ —Ñ–æ–ª—å–∫–ª–æ—Ä',\n",
        "#        '—Ñ–æ–ª—å–∫–ª–æ—Ä': '—ç—Ç–Ω–æ–∫—É–ª—å—Ç—É—Ä–∞ —Ñ–æ–ª—å–∫–ª–æ—Ä'\n",
        "#    }\n",
        "#    df['topic'] = df['topic'].replace(merge_map)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['topic'])\n",
        "\n",
        "# Train-validation split\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
        "print(f\"Training samples: {len(train_df)}, Validation samples: {len(val_df)}\")\n",
        "\n",
        "# Optional: you can also encode 'domain' and 'type_group' if you later want to use them\n",
        "# domain_encoder = LabelEncoder()\n",
        "# df['domain_id'] = domain_encoder.fit_transform(df['domain'])\n",
        "\n",
        "# Keep only necessary columns to save memory\n",
        "train_df = train_df[['text', 'label']].copy()\n",
        "val_df = val_df[['text', 'label']].copy()\n",
        "\n",
        "# Convert to HuggingFace Datasets\n",
        "train_ds = Dataset.from_pandas(train_df)\n",
        "val_ds = Dataset.from_pandas(val_df)\n",
        "datasets = DatasetDict({\"train\": train_ds, \"validation\": val_ds})\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenization\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n",
        "\n",
        "tokenized_datasets = datasets.map(preprocess_function, batched=True)\n",
        "\n",
        "# Load model with classification head\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\n",
        "\n",
        "# Metrics for evaluation\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    preds = np.argmax(predictions, axis=1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1_macro\": f1_score(labels, preds, average=\"macro\"),\n",
        "        \"f1_weighted\": f1_score(labels, preds, average=\"weighted\")\n",
        "    }\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_xlm_topic\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    per_device_eval_batch_size =per_device_eval_batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Free memory after training\n",
        "del tokenized_datasets\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Save model and tokenizer\n",
        "model.save_pretrained(\"./topic_model_finetuned\")\n",
        "tokenizer.save_pretrained(\"./topic_model_finetuned\")\n",
        "\n",
        "# Run inference on new unlabeled data\n",
        "from transformers import TextClassificationPipeline\n",
        "\n",
        "# Load unlabeled posts\n",
        "test_response = requests.get(f'https://raw.githubusercontent.com/componavt/sns4human/refs/heads/main/data/vk/posts/{unlabeled_posts}')\n",
        "test_df = pd.read_csv(StringIO(test_response.text), delimiter=',', encoding='utf-8')\n",
        "pipe = TextClassificationPipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    top_k=None,\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        "    truncation=True,              # üëà –æ–±—Ä–µ–∑–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã\n",
        "    max_length=max_length,               # üëà –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –º–∞–∫—Å–∏–º—É–º –¥–ª—è BERT\n",
        "    padding=True                  # üëà —á—Ç–æ–±—ã batch —Ä–∞–±–æ—Ç–∞–ª –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ\n",
        ")\n",
        "\n",
        "# Predict topic with score\n",
        "results = []\n",
        "for _, row in test_df.iterrows():\n",
        "    text = row['text']\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "      continue  # Skip invalid text entries\n",
        "\n",
        "    preds = pipe(text)[0]  # list of dicts [{label: 'LABEL_0', score: ...}, ...]\n",
        "    best = max(preds, key=lambda x: x['score'])\n",
        "    topic_label = label_encoder.inverse_transform([int(best['label'].split('_')[-1])])[0]\n",
        "    results.append({\n",
        "        \"id\": row.get('id', None),  # if 'id' exists\n",
        "\n",
        "        \"topic\": topic_label, # two calculated fields\n",
        "        \"relatedness\": round(best['score'], 4),\n",
        "\n",
        "        \"text\": text,\n",
        "        \"date\": row.get('date'),\n",
        "        \"likes\": row.get('likes'),\n",
        "        \"reposts\": row.get('reposts'),\n",
        "        \"views\": row.get('views'),\n",
        "    })\n",
        "\n",
        "# Save labeled results\n",
        "pd.DataFrame(results).to_csv(\"labeled_predictions.csv\", index=False, encoding=\"utf-8\")"
      ]
    }
  ]
}