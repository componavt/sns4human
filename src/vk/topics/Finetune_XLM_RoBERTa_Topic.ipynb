{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOqgN7Y92ASsp5DLFVcreOv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/componavt/sns4human/blob/main/src/vk/topics/Finetune_XLM_RoBERTa_Topic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# todo to Hugging Face Hub upload\n",
        "\n",
        "# Define input files\n",
        "file_expert_labeled = \"512_posts_24_topics.csv\"  # expert-labeled dataset\n",
        "unlabeled_posts = \"speechvepkar.csv\"\n",
        "\n",
        "# see 25 XLM-RoBERTa models: Russian + text classification: https://huggingface.co/models?pipeline_tag=text-classification&language=ru&sort=trending&search=XLM-RoBERTa\n",
        "\n",
        "model_name = \"xlm-roberta-base\" #üëå (125M+ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)\n",
        "# num_train_epochs=17 –ø—Ä–∏ 18 —Ç–æ—á–Ω–æ—Å—Ç—å –ø–µ—Ä–µ—Å—Ç–∞–ª–∞ —Ä–∞—Å—Ç–∏: 17 –∏ 18 —ç–ø–æ—Ö–∞: —Ç–æ—á–Ω–æ—Å—Ç—å 0.623656, 19 —ç–ø–æ—Ö–∞ - 0.634\n",
        "# OK: max_length=256\n",
        "# OK: per_device_train_batch_size=8\n",
        "# OK: per_device_eval_batch_size =8\n",
        "\n",
        "# model_name = \"mrm8488/XLM-RoBERTa-tiny\" # failed to load –£—Ä–µ–∑–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è XLM-R —Å –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π üîΩ 22–ú\n",
        "# model_name = \"mrm8488/xlm-roberta-base-finetuned-HC3-mix\" # –º–æ–¥–µ–ª—å —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—É—é \"–≥–æ–ª–æ–≤—É\" (head) —Å 2 –≤—ã—Ö–æ–¥–∞–º–∏, –ø–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω–∞ –æ–±—É—á–∞–ª–∞—Å—å –Ω–∞ –¥–≤–æ–∏—á–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, truthful / deceptive)\n",
        "# model_name = \"DeepPavlov/xlm-roberta-large-en-ru-mnli\" # (head) —Å 3 –≤—ã—Ö–æ–¥–∞–º–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏, —Ä–∞–∑–º–µ—Ä [3, 1024] (3 –∫–ª–∞—Å—Å–∞), –∞ –Ω–µ 23\n",
        "\n",
        "# model_name = \"cointegrated/rubert-tiny\" #üëå–ö—Ä–∞–π–Ω–µ –Ω–∏–∑–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ (Accuracy 21.5%, F1 ~4‚Äì9%) ‚Äî –º–æ–¥–µ–ª—å –ø–æ—á—Ç–∏ –Ω–µ –æ–±—É—á–∞–µ—Ç—Å—è.\n",
        "# model_name = \"distilbert-base-multilingual-cased\" #üëå–º–µ—Ç—Ä–∏–∫–∏ –Ω–µ–ø—Ä–∏–µ–º–ª–µ–º–æ –Ω–∏–∑–∫–∏–µ (Accuracy ~25%, F1 < 15%).\n",
        "                                                    # –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π DistilBERT —Å –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ—Å—Ç—å—é\tüîΩ 134–ú\n",
        "# model_name = \"papluca/xlm-roberta-base-language-detection\" # failed, since language-detection\n",
        "\n",
        "# –£–º–µ–Ω—å—à–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è 3, 2, 1\n",
        "num_train_epochs=17 # 21 # 3 10\n",
        "\n",
        "# –£–º–µ–Ω—å—à–∏—Ç—å max_length –ø—Ä–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (todo: –ø–æ–¥—Å—á–∏—Ç–∞—Ç—å –¥–ª–∏–Ω—É –ø–æ—Å—Ç–æ–≤)\n",
        "max_length=512\n",
        "# max_length=256\n",
        "# max_length=128\n",
        "\n",
        "learning_rate=2e-5 # (0.00002) –ú–µ–Ω—å—à–∏–π learning rate –æ–∑–Ω–∞—á–∞–µ—Ç –±–æ–ª–µ–µ –º–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏. –≠—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–º—É, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é.\n",
        "# learning_rate=3e-5 # (0.00003) –ß—É—Ç—å –±–æ–ª—å—à–∏–π learning rate —É—Å–∫–æ—Ä—è–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤, —á—Ç–æ –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å –º–æ–¥–µ–ª–∏ –±—ã—Å—Ç—Ä–µ–µ —Å—Ö–æ–¥–∏—Ç—å—Å—è, –Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —Ä–∏—Å–∫ \"–ø–µ—Ä–µ—Å–∫–∞–∫–∏–≤–∞–Ω–∏—è\" –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è.\n",
        "\n",
        "per_device_train_batch_size=16\n",
        "per_device_eval_batch_size =16\n",
        "#per_device_train_batch_size=8\n",
        "#per_device_eval_batch_size =8\n",
        "\n",
        "from config import hugging_face_username"
      ],
      "metadata": {
        "id": "DTPaOuoim6l6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune XLM-RoBERTa for topic classification on Russian social media posts (GPU-friendly üß†‚ö°)\n",
        "\n",
        "!pip install -U transformers datasets scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "from io import StringIO\n",
        "import requests\n",
        "\n",
        "# Load data labeled by expert from GitHub\n",
        "response = requests.get(f'https://raw.githubusercontent.com/componavt/sns4human/refs/heads/main/data/vk/topics/{file_expert_labeled}')\n",
        "df = pd.read_csv(StringIO(response.text), delimiter=',', encoding='utf-8')\n",
        "df = df[df['topic'].str.lower() != '–ø—É—Å—Ç–æ'].copy()\n",
        "\n",
        "#if model_name == \"papluca/xlm-roberta-base-language-detection\":\n",
        "    # Merge expert-assigned topics\n",
        "    # (1) –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤ –æ–¥–Ω—É —Ç–µ–º—É \"–ø–æ—ç–∑–∏—è –ö–∞–ª–µ–≤–∞–ª–∞ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞\" —Ç—Ä–∏ —Ç–µ–º—ã: \"–ø–æ—ç–∑–∏—è\", \"–≠–ø–æ—Å_–ö–∞–ª–µ–≤–∞–ª–∞\", \"–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞\";\n",
        "    # (2) –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤ –æ–¥–Ω—É —Ç–µ–º—É \"—ç—Ç–Ω–æ–∫—É–ª—å—Ç—É—Ä–∞ —Ñ–æ–ª—å–∫–ª–æ—Ä\" –¥–≤–µ —Ç–µ–º—ã: \"—ç—Ç–Ω–æ–∫—É–ª—å—Ç—É—Ä–∞\" –∏ \"—Ñ–æ–ª—å–∫–ª–æ—Ä\". –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ç–µ–º —Å—Ç–∞–Ω–µ—Ç –≤–º–µ—Å—Ç–æ 23 –±—É–¥–µ—Ç 20.\n",
        "#    merge_map = {\n",
        "#        '–ø–æ—ç–∑–∏—è': '–ø–æ—ç–∑–∏—è –ö–∞–ª–µ–≤–∞–ª–∞ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞',\n",
        "#        '–≠–ø–æ—Å_–ö–∞–ª–µ–≤–∞–ª–∞': '–ø–æ—ç–∑–∏—è –ö–∞–ª–µ–≤–∞–ª–∞ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞',\n",
        "#        '–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞': '–ø–æ—ç–∑–∏—è –ö–∞–ª–µ–≤–∞–ª–∞ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞',\n",
        "#        '—ç—Ç–Ω–æ–∫—É–ª—å—Ç—É—Ä–∞': '—ç—Ç–Ω–æ–∫—É–ª—å—Ç—É—Ä–∞ —Ñ–æ–ª—å–∫–ª–æ—Ä',\n",
        "#        '—Ñ–æ–ª—å–∫–ª–æ—Ä': '—ç—Ç–Ω–æ–∫—É–ª—å—Ç—É—Ä–∞ —Ñ–æ–ª—å–∫–ª–æ—Ä'\n",
        "#    }\n",
        "#    df['topic'] = df['topic'].replace(merge_map)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['topic'])\n",
        "\n",
        "# Train-validation split\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
        "print(f\"Training samples: {len(train_df)}, Validation samples: {len(val_df)}\")\n",
        "\n",
        "# Optional: you can also encode 'domain' and 'type_group' if you later want to use them\n",
        "# domain_encoder = LabelEncoder()\n",
        "# df['domain_id'] = domain_encoder.fit_transform(df['domain'])\n",
        "\n",
        "# Keep only necessary columns to save memory\n",
        "train_df = train_df[['text', 'label']].copy()\n",
        "val_df = val_df[['text', 'label']].copy()\n",
        "\n",
        "# Convert to HuggingFace Datasets\n",
        "train_ds = Dataset.from_pandas(train_df)\n",
        "val_ds = Dataset.from_pandas(val_df)\n",
        "datasets = DatasetDict({\"train\": train_ds, \"validation\": val_ds})\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenization\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n",
        "\n",
        "tokenized_datasets = datasets.map(preprocess_function, batched=True)\n",
        "\n",
        "# Load model with classification head\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\n",
        "\n",
        "# Metrics for evaluation\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    preds = np.argmax(predictions, axis=1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1_macro\": f1_score(labels, preds, average=\"macro\"),\n",
        "        \"f1_weighted\": f1_score(labels, preds, average=\"weighted\")\n",
        "    }\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_xlm_topic\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    per_device_eval_batch_size =per_device_eval_batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# ========== NEW: DYNAMIC QUANTIZATION TO INT8 ==========\n",
        "print(\"\\nApplying dynamic quantization to int8...\")\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model,\n",
        "    {torch.nn.Linear},  # Quantize only linear layers\n",
        "    dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Free memory after training\n",
        "del tokenized_datasets\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Save model and tokenizer\n",
        "model.save_pretrained(\"./topic_model_finetuned\")\n",
        "tokenizer.save_pretrained(\"./topic_model_finetuned\")"
      ],
      "metadata": {
        "id": "NhYs4qEfbrn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iGVGyYP4mrUm",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "689c2176-040c-422e-a3dc-f35230a27ce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Uploading model to Hugging Face Hub: componavt/xlm-roberta-base-topic-classification-2025\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'torch.dtype' object has no attribute 'data_ptr'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-c365c6dc70dc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Save model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mquantized_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36msave_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   3459\u001b[0m                                 \u001b[0mto_delete_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m             \u001b[0;31m# We are entering a place where the weights and the transformers configuration do NOT match.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3461\u001b[0;31m             \u001b[0mshared_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisjoint_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_disjoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshared_ptrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3462\u001b[0m             \u001b[0;31m# Those are actually tensor sharing but disjoint from each other, we can safely clone them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3463\u001b[0m             \u001b[0;31m# Reloaded won't have the same property, but it shouldn't matter in any meaningful way.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_find_disjoint\u001b[0;34m(tensors, state_dict)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshared\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m             \u001b[0mareas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_end_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0mareas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'torch.dtype' object has no attribute 'data_ptr'"
          ]
        }
      ],
      "source": [
        "# ========== COMPLETE FIXED UPLOAD SOLUTION ==========\n",
        "from huggingface_hub import HfApi, ModelCard\n",
        "from datetime import datetime\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create repository name (add timestamp for uniqueness)\n",
        "#repo_name = f\"{hugging_face_username}/xlm-roberta-base-topic-classification-{datetime.now().strftime('%Y%m%d')}\"\n",
        "repo_name = f\"{hugging_face_username}/xlm-roberta-base-topic-classification-2025\"\n",
        "print(f\"\\nUploading model to Hugging Face Hub: {repo_name}\")\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] =\"your token\"\n",
        "\n",
        "# 3. Save the quantized model in FP32 format first\n",
        "save_path = \"./model_for_upload\"\n",
        "quantized_model.config.torch_dtype = torch.float32  # Force FP32 format\n",
        "\n",
        "# Save model and tokenizer\n",
        "quantized_model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "# 4. Create repository\n",
        "api = HfApi()\n",
        "try:\n",
        "    # First create the repository\n",
        "    api.create_repo(\n",
        "        repo_id=repo_name,\n",
        "        repo_type=\"model\",\n",
        "        token=True,\n",
        "        exist_ok=True\n",
        "    )\n",
        "\n",
        "    # 5. Upload files manually (bypassing push_to_hub for quantized model)\n",
        "    api.upload_folder(\n",
        "        folder_path=save_path,\n",
        "        repo_id=repo_name,\n",
        "        repo_type=\"model\",\n",
        "        commit_message=\"Add quantized XLM-RoBERTa (int8)\",\n",
        "        token=True\n",
        "    )\n",
        "\n",
        "    # 6. Create and upload model card\n",
        "    card_content = f\"\"\"\n",
        "---\n",
        "language:\n",
        "- ru\n",
        "- multilingual\n",
        "tags:\n",
        "- text-classification\n",
        "- topic-modeling\n",
        "- xlm-roberta\n",
        "---\n",
        "\n",
        "# XLM-RoBERTa Base for Topic Classification (Quantized)\n",
        "\n",
        "Quantized (int8) version fine-tuned for Russian social media topic classification.\n",
        "- Trained on: {file_expert_labeled}\n",
        "- Epochs: {num_train_epochs}\n",
        "- Learning rate: {learning_rate}\n",
        "\"\"\"\n",
        "    with open(f\"{save_path}/README.md\", \"w\") as f:\n",
        "        f.write(card_content)\n",
        "\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=f\"{save_path}/README.md\",\n",
        "        path_in_repo=\"README.md\",\n",
        "        repo_id=repo_name,\n",
        "        token=True\n",
        "    )\n",
        "\n",
        "    print(\"‚úÖ Upload complete!\")\n",
        "    print(f\"Model available at: https://huggingface.co/{repo_name}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error during upload: {e}\")\n",
        "    raise\n",
        "finally:\n",
        "    # Clean up temporary files\n",
        "    shutil.rmtree(save_path, ignore_errors=True)\n",
        "    print(\"Temporary files cleaned up\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run inference on new unlabeled data\n",
        "from transformers import TextClassificationPipeline\n",
        "\n",
        "# Load unlabeled posts\n",
        "test_response = requests.get(f'https://raw.githubusercontent.com/componavt/sns4human/refs/heads/main/data/vk/posts/{unlabeled_posts}')\n",
        "test_df = pd.read_csv(StringIO(test_response.text), delimiter=',', encoding='utf-8')\n",
        "pipe = TextClassificationPipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    top_k=None,\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        "    truncation=True,              # üëà –æ–±—Ä–µ–∑–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã\n",
        "    max_length=max_length,               # üëà –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –º–∞–∫—Å–∏–º—É–º –¥–ª—è BERT\n",
        "    padding=True                  # üëà —á—Ç–æ–±—ã batch —Ä–∞–±–æ—Ç–∞–ª –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ\n",
        ")\n",
        "\n",
        "# Predict topic with score\n",
        "results = []\n",
        "for _, row in test_df.iterrows():\n",
        "    text = row['text']\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "      continue  # Skip invalid text entries\n",
        "\n",
        "    preds = pipe(text)[0]  # list of dicts [{label: 'LABEL_0', score: ...}, ...]\n",
        "    best = max(preds, key=lambda x: x['score'])\n",
        "    topic_label = label_encoder.inverse_transform([int(best['label'].split('_')[-1])])[0]\n",
        "    results.append({\n",
        "        \"id\": row.get('id', None),  # if 'id' exists\n",
        "\n",
        "        \"topic\": topic_label, # two calculated fields\n",
        "        \"relatedness\": round(best['score'], 4),\n",
        "\n",
        "        \"text\": text,\n",
        "        \"date\": row.get('date'),\n",
        "        \"likes\": row.get('likes'),\n",
        "        \"reposts\": row.get('reposts'),\n",
        "        \"views\": row.get('views'),\n",
        "    })\n",
        "\n",
        "# Save labeled results\n",
        "pd.DataFrame(results).to_csv(\"labeled_predictions.csv\", index=False, encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "Sg-Sev-6HNEB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}