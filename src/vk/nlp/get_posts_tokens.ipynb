{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/componavt/sns4human/blob/main/src/vk/nlp/get_posts_tokens.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script processes text data, tokenizes it and saves it to a CSV file, optionally taking into account the posts' affiliation to a specific VK group.\n",
        "\n",
        "Этот скрипт обрабатывает текстовые данные, токенизирует их и сохраняет в CSV-файл опционально с учетом принадлежности постов к конкретной ВК группе."
      ],
      "metadata": {
        "id": "BfoA10pXuUrx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PO6yySLtgsh2",
        "outputId": "1d96ddee-d519-410b-d2c7-68f0378aabc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (2.4.417150.4580142)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.14.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pymorphy3\n",
        "!pip install emoji\n",
        "import pandas as pd\n",
        "import requests\n",
        "import csv\n",
        "from io import StringIO\n",
        "filename = 'text_preprocessing.py'\n",
        "response = requests.get(f'https://raw.githubusercontent.com/componavt/sns4human/refs/heads/main/src/vk/nlp/{filename}')\n",
        "with open(filename,'w+') as f:\n",
        "  f.write(response.text)\n",
        "import text_preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# domains = ['club221681617','concerto','club151359929','pravoslav_karelia']# smallest groups for tests\n",
        "domains = ['aparfenchikov','minnazrk', 'mincultrk', 'uoknrk']# state_tokens.csv\n",
        "\n",
        "#folder = 'religion'\n",
        "folder = ''\n",
        "url = f'https://raw.githubusercontent.com/componavt/sns4human/refs/heads/main/data/vk/posts/{folder}'\n",
        "\n",
        "def tokens_of_posts(domains, url, flag_id_group=False):\n",
        "  if flag_id_group:\n",
        "    df = pd.concat([pd.read_csv(\n",
        "            f'{url}/{domain}.csv',\n",
        "            usecols=['text', 'date']\n",
        "        ).assign(group_id=domain)\n",
        "        for domain in domains\n",
        "    ], ignore_index=True).sort_values('date')\n",
        "\n",
        "  else:\n",
        "    df = pd.concat([pd.read_csv(\n",
        "            f'{url}/{domain}.csv',\n",
        "            usecols = ['text','date'])\n",
        "           for domain in domains],\n",
        "          ignore_index=True).sort_values('date')\n",
        "  df = df[df['text'].notna() & df['text'].astype(str).str.strip().astype(bool)].reset_index(drop=True)\n",
        "\n",
        "  \"\"\"\n",
        "  Используется функция process_text() из модуля text_preprocessing, которая принимает на вход два параметра:\n",
        "  text - предложения в формате строки для обработки\n",
        "  points - параметр по умолчанию, чтобы получить токенизацию формата, необходимого для Voyants Tools, нужно вызвать функция с явным указанием параметра в значении True\n",
        "  Тогда будет сгененрирован файл с точечной периодизацией\n",
        "  Пример для Voyant Tools: process_text(text, True)\n",
        "  Пример для тематического моделирования process_text(text)\n",
        "\n",
        "  \"\"\"\n",
        "  df['tokens'] = df['text'].apply(text_preprocessing.process_text)\n",
        "  # Removing lines with empty 'tokens'\n",
        "  df = df[df['tokens'].str.strip().astype(bool)]\n",
        "\n",
        "  # Save CSV with quotes only for 'tokens' field, without quotes for 'date'\n",
        "  with open('tokens_with_group_id.csv', 'w', newline='', encoding='utf-8') as f:\n",
        "    writer = csv.writer(f, delimiter=';', quoting=csv.QUOTE_NONE, quotechar=None)\n",
        "    if flag_id_group:\n",
        "      writer.writerow(['tokens', 'date','group_id'])\n",
        "      for _, row in df.iterrows():\n",
        "        writer.writerow([f'\"{row[\"tokens\"]}\"', row['date'], f'\"{row[\"group_id\"]}\"'])\n",
        "    else:\n",
        "      writer.writerow(['tokens', 'date'])\n",
        "      for _, row in df.iterrows():\n",
        "        writer.writerow([f'\"{row[\"tokens\"]}\"', row['date']])\n",
        "  return df\n",
        "\n",
        "\"\"\"\n",
        "  Используется функция tokens_of_posts(), которая принимает на вход три параметра:\n",
        "  domains - id групп в формате списка строк\n",
        "  url - ссылка расположения файла с постами\n",
        "  flag_id_group - параметр по умолчанию, чтобы получить файл со столбцом 'group_id, необходимо при вызове явно указать параметр в значении True\n",
        "  Пример для генерации файла с колонкой group_id: tokens_of_posts(domains,url,True)\n",
        "\"\"\"\n",
        "df = tokens_of_posts(domains,url)\n"
      ],
      "metadata": {
        "id": "1l1PJxzg0gEn"
      },
      "execution_count": 4,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}