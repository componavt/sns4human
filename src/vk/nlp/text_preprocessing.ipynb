{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAKbY0ZS86I8OQRfiLvZd1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/componavt/sns4human/blob/main/src/vk/nlp/text_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script preprocesses Russian-language text:\n",
        "Splits the text into sentences and words (removing hashtags)\n",
        "\n",
        "*   Removes characters outside the Russian alphabet\n",
        "*   Excludes stop words (built-in + custom lists from GitHub)\n",
        "*   Filters out emoji, URLs and single-letter tokens\n",
        "\n",
        "*   Normalizes words via PyMorphy3 (reduces to the initial form)\n",
        "*   Optionally adds punctuation marks (dots)\n",
        "*   Returns a cleaned and lemmatized string.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Этот скрипт выполняет предобработку русскоязычного текста:\n",
        "Разбивает текст на предложения и слова (удаляя хэштеги)\n",
        "\n",
        "*    Удаляет символы вне русского алфавита\n",
        "\n",
        "*   Исключает стоп-слова (встроенные + кастомные списки с GitHub)\n",
        "*  Отфильтровывает эмодзи, URL и однобуквенные токены\n",
        "*  Нормализует слова через PyMorphy3 (приводит к начальной форме)\n",
        "*  Опционально добавляет знаки препинания (точки)\n",
        "*  Возвращает очищенную и лемматизированную строку.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZHJP2VLTatny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import pymorphy3\n",
        "import requests\n",
        "import csv\n",
        "import emoji\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "\n",
        "stop_words = stopwords.words(\"russian\")\n",
        "stop_words += requests.get('https://raw.githubusercontent.com/componavt/sns4human/refs/heads/main/src/vk/nlp/RussianStopWords.txt').text.split('\\n')\n",
        "stop_words += requests.get('https://raw.githubusercontent.com/componavt/sns4human/refs/heads/main/src/vk/nlp/stopwords-ru.txt').text.split()\n",
        "\n",
        "alphabet = set('абвгдеёжзийклмнопрстуфхцчшщъыьэюя-.')\n",
        "morph = pymorphy3.MorphAnalyzer(lang='ru')"
      ],
      "metadata": {
        "id": "qdHP0SXRrXRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4cyT6RRa7Qu"
      },
      "outputs": [],
      "source": [
        "def process_text(text, points=False):\n",
        "    sentences = sent_tokenize(text)\n",
        "    processed_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        processed_parts = []\n",
        "        tokenize_sent = [t for t in word_tokenize(sentence) if not t.startswith('#')]\n",
        "        for w in word_tokenize(tokenize_sent):\n",
        "            if (len(w) == 1 or\n",
        "                \":\" in emoji.demojize(w) or\n",
        "                not set(w.lower()).issubset(alphabet) or\n",
        "                not w[0:2].lower().isalpha() or\n",
        "                '\\\\' in w or\n",
        "                '/' in w):\n",
        "                continue\n",
        "\n",
        "            res = morph.parse(w.lower())[0].normal_form\n",
        "            if res and (res not in stop_words):\n",
        "                processed_parts.append(res)\n",
        "\n",
        "        if points and processed_parts:\n",
        "            processed_parts[-1] += \".\"\n",
        "\n",
        "        processed_sentences.append(\" \".join(processed_parts))\n",
        "\n",
        "    return \" \".join(processed_sentences)"
      ]
    }
  ]
}