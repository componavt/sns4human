{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/componavt/sns4human/blob/main/src/vk/nlp/get_posts_tokens_with_group_id.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script processes text data, tokenizes it and saves it to a CSV file, taking into account the posts' affiliation to a specific VK group.\n",
        "\n",
        "Этот скрипт обрабатывает текстовые данные, токенизирует их и сохраняет в CSV-файл с учетом принадлежности постов к конкретной ВК группе."
      ],
      "metadata": {
        "id": "BfoA10pXuUrx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO6yySLtgsh2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "!pip install -U pymorphy3\n",
        "import pymorphy3\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words(\"russian\")\n",
        "alphabet = set('абвгдеёжзийклмнопрстуфхцчшщъыьэюя-')\n",
        "morph = pymorphy3.MorphAnalyzer(lang='ru')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "domains = ['satasanaa', 'speechvepkar', 'desyatiletieyazykovkarelia', 'learning_karelian_language']\n",
        "\n",
        "temp_df = []\n",
        "for domain in domains:\n",
        "    t = pd.read_csv(f'https://raw.githubusercontent.com/componavt/sns4human/refs/heads/main/data/vk/posts/{domain}.csv', usecols=['text', 'date'])\n",
        "    t['group_name'] = domain\n",
        "    temp_df.append(t)\n",
        "\n",
        "df = pd.concat(temp_df, ignore_index=True).sort_values('date')\n",
        "df = df[df['text'].notna() & (df['text'].apply(lambda x: isinstance(x, str))) & (df['text'] != '') ]"
      ],
      "metadata": {
        "id": "1l1PJxzg0gEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjsktO3-kRBW"
      },
      "outputs": [],
      "source": [
        "def process_text(text):\n",
        "    check_hash = False\n",
        "    processed_parts = []\n",
        "    for w in nltk.word_tokenize(text):\n",
        "      if len(w) == 1:\n",
        "        continue\n",
        "      if w == '#':\n",
        "          check_hash = True\n",
        "          continue\n",
        "      if check_hash:\n",
        "          check_hash = False\n",
        "          continue\n",
        "      w_tag = morph.parse(w.strip())[0].tag\n",
        "      if   'Surn' in w_tag or 'Name' in w_tag or 'Patr' in w_tag:\n",
        "         continue\n",
        "      if set(w.lower()).issubset(alphabet):\n",
        "        if w.isalpha() and w.lower():\n",
        "          if w.isupper() and len(w) <= 3:\n",
        "              processed_parts.append(w)\n",
        "          else:\n",
        "              res = morph.parse(w.lower())[0].normal_form\n",
        "              if res not in stop_words:\n",
        "                  processed_parts.append(res)\n",
        "    result = ' '.join(processed_parts)\n",
        "    return str(result)\n",
        "\n",
        "df['tokens'] = df['text'].apply(lambda x: process_text(x))\n",
        "df = df[df['tokens'].notna() & (df['tokens'].apply(lambda x: isinstance(x, str))) & (df['tokens'] != '') & (df['tokens'] != ' ')]\n",
        "df_tokens = pd.concat([df['tokens'], df['date'],df['group_name']], axis=1, keys=['tokens', 'date','group_name'])\n",
        "\n",
        "df_tokens.to_csv('tokens.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3yaWXWBQab0Tvnpr10xyg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}