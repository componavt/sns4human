{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/componavt/sns4human/blob/main/src/vk/nlp/get_posts_tokens_wtith_group_id.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PO6yySLtgsh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bafa24a2-3eb9-46d6-cde2-02053f034072"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (2.4.417150.4580142)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "!pip install -U pymorphy3\n",
        "import pymorphy3\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words(\"russian\")\n",
        "alphabet = set('абвгдеёжзийклмнопрстуфхцчшщъыьэюя-')\n",
        "morph = pymorphy3.MorphAnalyzer(lang='ru')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#domains = ['rk_nationalmuseum','olonmus','museum_ptz','echo_association','domderevnivoknavolok']\n",
        "\n",
        "domains = ['rk_nationalmuseum', 'olonmus']\n",
        "\n",
        "temp_df = []\n",
        "for domain in domains:\n",
        "    t = pd.read_csv(f'https://raw.githubusercontent.com/componavt/sns4human/refs/heads/main/data/vk/posts/{domain}.csv', usecols=['text', 'date'])\n",
        "    t['group_name'] = domain\n",
        "    temp_df.append(t)\n",
        "\n",
        "df = pd.concat(temp_df, ignore_index=True).sort_values('date')\n",
        "df = df[df['text'].notna() & (df['text'].apply(lambda x: isinstance(x, str))) & (df['text'] != '') ]"
      ],
      "metadata": {
        "id": "1l1PJxzg0gEn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EjsktO3-kRBW"
      },
      "outputs": [],
      "source": [
        "def process_text(text):\n",
        "    check_hash = False\n",
        "    processed_parts = []\n",
        "    for w in nltk.word_tokenize(text):\n",
        "      if len(w) == 1:\n",
        "        continue\n",
        "      if w == '#':\n",
        "          check_hash = True\n",
        "          continue\n",
        "      if check_hash:\n",
        "          check_hash = False\n",
        "          continue\n",
        "      w_tag = morph.parse(w.strip())[0].tag\n",
        "      if   'Surn' in w_tag or 'Name' in w_tag or 'Patr' in w_tag:\n",
        "         continue\n",
        "      if set(w.lower()).issubset(alphabet):\n",
        "        if w.isalpha() and w.lower():\n",
        "          if w.isupper() and len(w) <= 3:\n",
        "              processed_parts.append(w)\n",
        "          else:\n",
        "              res = morph.parse(w.lower())[0].normal_form\n",
        "              if res not in stop_words:\n",
        "                  processed_parts.append(res)\n",
        "    result = ' '.join(processed_parts)\n",
        "    return str(result)\n",
        "\n",
        "df['tokens'] = df['text'].apply(lambda x: process_text(x))\n",
        "df = df[df['tokens'].notna() & (df['tokens'].apply(lambda x: isinstance(x, str))) & (df['tokens'] != '') & (df['tokens'] != ' ')]\n",
        "df_tokens = pd.concat([df['tokens'], df['date'],df['group_name']], axis=1, keys=['tokens', 'date','group_name'])\n",
        "\n",
        "df_tokens.to_csv('tokens.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOPOMx8LJtk6cB3PIIUQfi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
