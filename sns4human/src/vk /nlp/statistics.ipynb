{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/componavt/sns4human/blob/main/sns4human/src/vk%20/nlp/statistics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6JkcjAZVsr3"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "!python -m spacy download ru_core_news_sm\n",
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "stop_words = stopwords.words(\"russian\")\n",
        "!pip install -U pymorphy2==0.9.1\n",
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer(lang='ru')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Zy12gdVl3nMC"
      },
      "outputs": [],
      "source": [
        "posts = {\n",
        "         'multifunctional': ['https://raw.githubusercontent.com/componavt/sns4human/refs/heads/main/data/vk/posts/karjalankielenkodi.csv',\n",
        "                      'https://raw.githubusercontent.com/componavt/sns4human/refs/heads/main/data/vk/posts/mediacenter_periodika.csv',\n",
        "                      'https://raw.githubusercontent.com/componavt/sns4human/refs/heads/main/data/vk/posts/кarelian_flavor.csv',\n",
        "                      'https://raw.githubusercontent.com/componavt/sns4human/refs/heads/main/data/vk/posts/karjalanrahvahanliitto.csv',\n",
        "                      'https://raw.githubusercontent.com/componavt/sns4human/refs/heads/main/data/vk/posts/karelian_speakers.csv'],\n",
        "         }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "obaEF7PsaJvR"
      },
      "outputs": [],
      "source": [
        "def process_text(text, index, df):\n",
        "    check_hash = False\n",
        "    processed_parts = []\n",
        "    for w in nltk.word_tokenize(text):\n",
        "      if w == '#':\n",
        "          check_hash = True\n",
        "          continue\n",
        "      if check_hash:\n",
        "          check_hash = False\n",
        "          continue\n",
        "      w_tag = morph.parse(w.strip())[0].tag\n",
        "      if  'Surn' in w_tag or 'Name' in w_tag or 'Patr' in w_tag:\n",
        "        continue\n",
        "      if w.isalpha() and w.lower() not in stop_words:\n",
        "        if w.isupper() and len(w) <= 3:\n",
        "            processed_parts.append(w)\n",
        "        else:\n",
        "            processed_parts.append(morph.parse(w.lower())[0].normal_form)\n",
        "\n",
        "    result = ' '.join(processed_parts)\n",
        "    df.at[index, 'tokens'] = str(result)\n",
        "\n",
        "\n",
        "urls = []\n",
        "names = []\n",
        "groups_posts = []\n",
        "tokens = []\n",
        "unique_words = []\n",
        "mid_len_post = []\n",
        "\n",
        "for key in posts.keys():\n",
        "    for url in posts[key]:\n",
        "      urls.append(url)\n",
        "      names.append(url.split('/')[-1].rstrip('.csv'))\n",
        "      t = 0\n",
        "      dict_lem = {}\n",
        "      df = pd.read_csv(url, usecols = ['text'],encoding='utf-8')\n",
        "      df = df[df['text'].apply(lambda x: isinstance(x, str))]\n",
        "      groups_posts.append(len(df))\n",
        "      for index, row in df.iterrows():\n",
        "        process_text(row['text'],index, df)\n",
        "      df.to_excel(url.split('/')[-1].rstrip('.csv')+'_df_tokens.xlsx', index=False)\n",
        "      df_tokens = df['tokens']\n",
        "      for row in df_tokens:\n",
        "        row_split = row.split()\n",
        "\n",
        "        for lemma in row_split:\n",
        "          if dict_lem.get(lemma, 0) == 0:\n",
        "              dict_lem[lemma] = 1\n",
        "          else:\n",
        "              dict_lem[lemma] += 1\n",
        "      data = df['tokens'].tolist()\n",
        "      for d in data:\n",
        "        if type(d) == str:\n",
        "          t += len(d.split())\n",
        "      mid_len_post.append(t // len(df))\n",
        "      dict_lem = dict(sorted(dict_lem.items(), key=lambda item: item[1], reverse=True))\n",
        "      tokens.append(t)\n",
        "      unique_words.append(len(dict_lem))\n",
        "      t = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ze9P-QXi9k7E"
      },
      "outputs": [],
      "source": [
        "d = {'Группа ВК': names, 'URL': urls, 'Количество постов': groups_posts, 'Количество токенов': tokens, 'Количество уникальных слов': unique_words, 'Средняя длина поста в словах': mid_len_post}\n",
        "df = pd.DataFrame(data=d)\n",
        "df.to_csv('statistics.csv', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-Q9OPvR9LpI"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjoSS0kXbBZmSldI1zROpn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}